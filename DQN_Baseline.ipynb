{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Oy-J7wZctbU"
   },
   "source": [
    "### **SOW-MKI49-2019-SEM1-V: NeurIPS**\n",
    "#### Project: Neurosmash\n",
    "\n",
    "This is the info document on the (updated*) Neurosmash environment that you will be using for your project. It contains background info and skeleton code to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snucT1NJctbe"
   },
   "source": [
    "### Project\n",
    "\n",
    "In the next 4 + 1 weeks, you will be working exclusively on your project in the practicals. The goal is to take what has been discussed in class and what you have already worked on in the earlier practicals, and apply them on a RL problem in a novel environment. Note that while the earlier practicals were intended to give you the opportunity to gain experience with various RL topics and were not graded, your project will constitute 50% of your final grade.\n",
    "\n",
    "Your project grade will be based on the following components:\n",
    "- Online demonstration\n",
    "- Source code\n",
    "- Written report\n",
    "\n",
    "These components will be evaluated based on performance, creativity, elegance, rigor and plausibility.\n",
    "\n",
    "While you can use the material from earlier practicals (e.g., REINFORCE, DQN, etc.) as a boilerplate, you are also free to take any other approach be it imitation learning or world models for your project.\n",
    "\n",
    "In addition to the practical sessions, we will provide additional support in the coming four weeks. You can email any of us to set up an appointment for discussing your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zw7RBTGctbi"
   },
   "source": [
    "### Environment\n",
    "\n",
    "Briefly, there are two agents: Red and Blue. Red is controlled by you. Blue is controlled by the environment \"AI\".* Both agents always run forward with a speed of 3.5 m/s*. If one of them gets within the reach of the other (a frontal sphere with 0.5 m radius), it gets pushed away automatically with a speed of 3.5 m/s. The only thing that the agents can do is to turn left or right with an angular speed of 180 degrees/s. This means that there are three possible discrete actions that your agent can take every step: Turn nowhere, turn left and turn right. For convenience, there is also a fourth built-in action which turns left or right with uniform probability. An episode begins when you reset the environment and ends when one of the agents fall off the platform. At the end of the episode, the winning agent gets a reward of 10 while the other gets nothing. Therefore, your goal is to train an agent who can maximize its reward by pushing the other agent off the platform or making it fall off the platform by itself.\n",
    "\n",
    "* None that all times are simulation time. That is, 0.02 s per step when timescale is set to one.\n",
    "\n",
    "* Basically, Blue is artificial but not really intelligent. What it does is that every 0.5 s, it updates its destination to the current position of Red plus some random variation (a surrounding circle with a radius of 1.75 m) and smoothly turns to that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owXhCqD0ctbl"
   },
   "source": [
    "### *Updates\n",
    "\n",
    "* There has been several small changes made to the lite version based on your feedback. Most notable ones are:\n",
    "- Bugs have (hopefully) been completely eliminated. Any remaining bug/glitch that your agent \"learns\" exploit will be considered fair game.\n",
    "\n",
    "- TCP/IP interface has been made more robust (you can now stop and start the simulation with the gui. no need to quit and rerun the environment anymore to reset it if something goes wrong.)\n",
    "- Animations/graphics have been updated (you can now tell what is going on more easily. agents actually fall down, etc.)\n",
    "- Last but not least, size and timescale settings have been added (you can now change the resolution and the speed of the environment to make the simulation run faster). In other words:\n",
    "\n",
    "Size => This is the size of the texture that the environment is rendered. This is set to 784 by default, which will result in a crisp image but slow speed. You can change the size to a value that works well for your environment should not go too low.\n",
    "\n",
    "Timescale => This is the simulation speed of the environment. This is set to 1 by default. Setting it to n will make the simulation n times faster. In other words, less (if n < 1) or more (if n > 1) simulation time will pass per step. You might want to increase this value to around 10 if you cannot train your models fast enough so that they can sample more states in a shorter number of steps at the expense of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aK1Ghyj1ctbp"
   },
   "source": [
    "### Misc. FAQs\n",
    "\n",
    "Q: Can we get HCP access?  \n",
    "A: I will try provide access to the AI HPC cluster if you require additional resources. If this is something that you would like, please contact me. Note however that you should use the cluster for training your final model and not development.\n",
    "\n",
    "Q: Will the environment code be shared?  \n",
    "A: Yes. I will share the entire unityproject at the end of the course (but without the 3D agent models).\n",
    "\n",
    "Q: Is there a environment version that can be played with a mouse/keyboard?  \n",
    "A: No but I will make one and update Brightspace when I have some free time.\n",
    "\n",
    "Q: I found a bug/glitch. What should I do?  \n",
    "A: Please let me know and I will fix it. Do note however that any updates from this point on will be optional to adopt. That is, you can keep working on the current environment if you so wish or think that updating will disadvantage you in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9e2gG4Yctbw"
   },
   "source": [
    "### Skeleton code\n",
    "\n",
    "- You should first add the Neurosmash file to your working directory or Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKI7Bb8agy_Z",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import socket\n",
    "from PIL import Image, ImageChops\n",
    "import struct\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Neurosmash_Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, end, reward, state):\n",
    "        # return 0 # nothing\n",
    "        # return 1 # left\n",
    "        # return 2 # right\n",
    "        return   2 # random\n",
    "\n",
    "class Neurosmash_Environment:\n",
    "    def __init__(self, ip = \"127.0.0.1\", port = 13000, size = 768, timescale = 1):\n",
    "        self.client     = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.ip         = ip\n",
    "        self.port       = port\n",
    "        self.size       = size\n",
    "        self.timescale  = timescale\n",
    "\n",
    "        self.client.connect((ip, port))\n",
    "\n",
    "    def reset(self):\n",
    "        self._send(1, 0)\n",
    "        return self._receive()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._send(2, action)\n",
    "        return self._receive()\n",
    "\n",
    "    def state2image(self, state):\n",
    "        return Image.fromarray(np.array(state, \"uint8\").reshape(self.size, self.size, 3))\n",
    "\n",
    "    def _receive(self):\n",
    "        # Kudos to Jan for the socket.MSG_WAITALL fix!\n",
    "        data   = self.client.recv(2 + 3 * self.size ** 2, socket.MSG_WAITALL)\n",
    "        end    = data[0]\n",
    "        reward = data[1]\n",
    "        state  = [data[i] for i in range(2, len(data))]\n",
    "\n",
    "        return end, reward, state\n",
    "\n",
    "    def _send(self, action, command):\n",
    "        self.client.send(bytes([action, command]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LrQ9Wpkctbz",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# These are the default environment arguments. They must be the same as the values that are set in the environment GUI.\n",
    "ip         = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "port       = 13000       # Port number that the TCP/IP interface listens to\n",
    "size       = 50         # Please check the Updates section above for more details\n",
    "timescale  = 10           # Please check the Updates section above for more details\n",
    "\n",
    "agent = Neurosmash_Agent() # This is an example agent.\n",
    "                           # It has a step function, which gets reward/state as arguments and returns an action.\n",
    "                           # Right now, it always outputs a random action (3) regardless of reward/state.\n",
    "                           # The real agent should output one of the following three actions:\n",
    "                           # none (0), left (1) and right (2)\n",
    "\n",
    "environment = Neurosmash_Environment(ip=ip, port=port, size=size, timescale=timescale)\n",
    "                                       # This is the main environment.\n",
    "                                       # It has a reset function, which is used to reset the environment before episodes.\n",
    "                                       # It also has a step function, which is used to which steps one time point\n",
    "                                       # It gets an action (as defined above) as input and outputs the following:\n",
    "                                       # end (true if the episode has ended, false otherwise)\n",
    "                                       # reward (10 if won, 0 otherwise)\n",
    "                                       # state (flattened size x size x 3 vector of pixel values)\n",
    "                                       # The state can be converted into an image as follows:\n",
    "                                       # image = np.array(state, \"uint8\").reshape(size, size, 3)\n",
    "                                       # You can also use to Neurosmash.Environment.state2image(state) function which returns\n",
    "                                       # the state as a PIL image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\" Container class for storing replay sequences, used in in DQN state replay \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Initialize environment, gather initial state\n",
    "end, reward, state = environment.reset()\n",
    "\n",
    "screen = environment.state2image(state)\n",
    "plt.imshow(screen)\n",
    "screen_shape = np.shape(screen)\n",
    "n_actions = 3\n",
    "\n",
    "#TODO: make a direct state2input function\n",
    "def image2input(screen):\n",
    "    \"\"\" Converts an image gained from the environment (through environment.state2image) to ready-to-use input\"\"\"\n",
    "    # Transpose to pytorch order of dimensions (cwh)\n",
    "    screen = np.transpose(screen, axes=(2,0,1))\n",
    "    # Translate to rgb float values\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Convert to tensor\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Add batch dimension, yeet to device\n",
    "    screen = screen.unsqueeze(0).to(device)\n",
    "    return screen\n",
    "    \n",
    "\n",
    "# Initialize DQN network\n",
    "policy_net = DQN(screen_shape[0], screen_shape[1], n_actions).to(device)\n",
    "\n",
    "\"\"\"De-comment and provide correct file to load from saved model\"\"\"\n",
    "#policy_net.load_state_dict(torch.load(\"dict_e3500.kek\"))\n",
    "\n",
    "target_net = DQN(screen_shape[0], screen_shape[1], n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"Let the policy network determine the best action to take in the given state\"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "# def plot_durations():\n",
    "#     plt.figure(2)\n",
    "#     plt.clf()\n",
    "#     durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "#     plt.title('Training...')\n",
    "#     plt.xlabel('Episode')\n",
    "#     plt.ylabel('Duration')\n",
    "#     plt.plot(durations_t.numpy())\n",
    "#     # Take 100 episode averages and plot them too\n",
    "#     if len(durations_t) >= 100:\n",
    "#         means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "#         means = torch.cat((torch.zeros(99), means))\n",
    "#         plt.plot(means.numpy())\n",
    "\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "#     if is_ipython:\n",
    "#         display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        \n",
    "def plot_win_average():\n",
    "    \"\"\"\n",
    "    Crude method to plot average winning rate on a 10-episode running average.\n",
    "    Output is meaningless/incorrect for the first 5 episodes, as well as for the 5 most recent episodes, \n",
    "    as function tries to average into a nonexisting past/future.\n",
    "    \"\"\"\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "    means = []\n",
    "    for i in range(len(wins)):\n",
    "        if i>5:\n",
    "            means.append(sum(wins[i-5:i+5])/10)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(means)\n",
    "    \n",
    "    ax.set(xlabel='episodes', ylabel='average win rate',\n",
    "           title='Average rate')\n",
    "    ax.set_ylim(0, 11)\n",
    "    ax.set_xlim(0, i_episode)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.pause(0.001)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "steps_done = 0\n",
    "save_interval = 200\n",
    "\n",
    "wins = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    timer = time.time()\n",
    "    \n",
    "    # Initialize the environment and state\n",
    "    end, reward, state = environment.reset()\n",
    "    state = image2input(environment.state2image(state))\n",
    "    last_state = state\n",
    "    current_state = state\n",
    "    state = current_state - last_state\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        end, reward, state = environment.step(action.item())\n",
    "        if reward == 10:\n",
    "            print(\"Victory!!!!\")\n",
    "        state = image2input(environment.state2image(state))\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_state = current_state\n",
    "        current_state = state\n",
    "        if not end:\n",
    "            next_state = current_state - last_state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        \n",
    "        if end:         \n",
    "            # Record wins and losses in order to plot results\n",
    "            if reward >= 10:\n",
    "                wins.append(10.0)\n",
    "            else:\n",
    "                wins.append(0.0)\n",
    "                \n",
    "            episode_durations.append(t + 1)\n",
    "            #plot_durations()\n",
    "            plot_win_average()\n",
    "            break\n",
    "            \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    print(\"episode\", i_episode, \"completed in\", time.time() - timer)\n",
    "    \n",
    "    # Save network at each save_interval\n",
    "    if (i_episode+1)save_interval == 0:\n",
    "        torch.save(policy_net.state_dict(), \"dict_e\" + str(i_episode+1) +\".kek\")\n",
    "        \n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "# env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "README.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
