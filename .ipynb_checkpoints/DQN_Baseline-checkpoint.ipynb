{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Oy-J7wZctbU"
   },
   "source": [
    "### **SOW-MKI49-2019-SEM1-V: NeurIPS**\n",
    "#### Project: Neurosmash\n",
    "\n",
    "This is the info document on the (updated*) Neurosmash environment that you will be using for your project. It contains background info and skeleton code to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snucT1NJctbe"
   },
   "source": [
    "### Project\n",
    "\n",
    "In the next 4 + 1 weeks, you will be working exclusively on your project in the practicals. The goal is to take what has been discussed in class and what you have already worked on in the earlier practicals, and apply them on a RL problem in a novel environment. Note that while the earlier practicals were intended to give you the opportunity to gain experience with various RL topics and were not graded, your project will constitute 50% of your final grade.\n",
    "\n",
    "Your project grade will be based on the following components:\n",
    "- Online demonstration\n",
    "- Source code\n",
    "- Written report\n",
    "\n",
    "These components will be evaluated based on performance, creativity, elegance, rigor and plausibility.\n",
    "\n",
    "While you can use the material from earlier practicals (e.g., REINFORCE, DQN, etc.) as a boilerplate, you are also free to take any other approach be it imitation learning or world models for your project.\n",
    "\n",
    "In addition to the practical sessions, we will provide additional support in the coming four weeks. You can email any of us to set up an appointment for discussing your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zw7RBTGctbi"
   },
   "source": [
    "### Environment\n",
    "\n",
    "Briefly, there are two agents: Red and Blue. Red is controlled by you. Blue is controlled by the environment \"AI\".* Both agents always run forward with a speed of 3.5 m/s*. If one of them gets within the reach of the other (a frontal sphere with 0.5 m radius), it gets pushed away automatically with a speed of 3.5 m/s. The only thing that the agents can do is to turn left or right with an angular speed of 180 degrees/s. This means that there are three possible discrete actions that your agent can take every step: Turn nowhere, turn left and turn right. For convenience, there is also a fourth built-in action which turns left or right with uniform probability. An episode begins when you reset the environment and ends when one of the agents fall off the platform. At the end of the episode, the winning agent gets a reward of 10 while the other gets nothing. Therefore, your goal is to train an agent who can maximize its reward by pushing the other agent off the platform or making it fall off the platform by itself.\n",
    "\n",
    "* None that all times are simulation time. That is, 0.02 s per step when timescale is set to one.\n",
    "\n",
    "* Basically, Blue is artificial but not really intelligent. What it does is that every 0.5 s, it updates its destination to the current position of Red plus some random variation (a surrounding circle with a radius of 1.75 m) and smoothly turns to that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owXhCqD0ctbl"
   },
   "source": [
    "### *Updates\n",
    "\n",
    "* There has been several small changes made to the lite version based on your feedback. Most notable ones are:\n",
    "- Bugs have (hopefully) been completely eliminated. Any remaining bug/glitch that your agent \"learns\" exploit will be considered fair game.\n",
    "\n",
    "- TCP/IP interface has been made more robust (you can now stop and start the simulation with the gui. no need to quit and rerun the environment anymore to reset it if something goes wrong.)\n",
    "- Animations/graphics have been updated (you can now tell what is going on more easily. agents actually fall down, etc.)\n",
    "- Last but not least, size and timescale settings have been added (you can now change the resolution and the speed of the environment to make the simulation run faster). In other words:\n",
    "\n",
    "Size => This is the size of the texture that the environment is rendered. This is set to 784 by default, which will result in a crisp image but slow speed. You can change the size to a value that works well for your environment should not go too low.\n",
    "\n",
    "Timescale => This is the simulation speed of the environment. This is set to 1 by default. Setting it to n will make the simulation n times faster. In other words, less (if n < 1) or more (if n > 1) simulation time will pass per step. You might want to increase this value to around 10 if you cannot train your models fast enough so that they can sample more states in a shorter number of steps at the expense of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aK1Ghyj1ctbp"
   },
   "source": [
    "### Misc. FAQs\n",
    "\n",
    "Q: Can we get HCP access?  \n",
    "A: I will try provide access to the AI HPC cluster if you require additional resources. If this is something that you would like, please contact me. Note however that you should use the cluster for training your final model and not development.\n",
    "\n",
    "Q: Will the environment code be shared?  \n",
    "A: Yes. I will share the entire unityproject at the end of the course (but without the 3D agent models).\n",
    "\n",
    "Q: Is there a environment version that can be played with a mouse/keyboard?  \n",
    "A: No but I will make one and update Brightspace when I have some free time.\n",
    "\n",
    "Q: I found a bug/glitch. What should I do?  \n",
    "A: Please let me know and I will fix it. Do note however that any updates from this point on will be optional to adopt. That is, you can keep working on the current environment if you so wish or think that updating will disadvantage you in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9e2gG4Yctbw"
   },
   "source": [
    "### Skeleton code\n",
    "\n",
    "- You should first add the Neurosmash file to your working directory or Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKI7Bb8agy_Z",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import socket\n",
    "from PIL import Image, ImageChops\n",
    "import struct\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Neurosmash_Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, end, reward, state):\n",
    "        # return 0 # nothing\n",
    "        # return 1 # left\n",
    "        # return 2 # right\n",
    "        return   2 # random\n",
    "\n",
    "class Neurosmash_Environment:\n",
    "    def __init__(self, ip = \"127.0.0.1\", port = 13000, size = 768, timescale = 1):\n",
    "        self.client     = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.ip         = ip\n",
    "        self.port       = port\n",
    "        self.size       = size\n",
    "        self.timescale  = timescale\n",
    "\n",
    "        self.client.connect((ip, port))\n",
    "\n",
    "    def reset(self):\n",
    "        self._send(1, 0)\n",
    "        return self._receive()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._send(2, action)\n",
    "        return self._receive()\n",
    "\n",
    "    def state2image(self, state):\n",
    "        return Image.fromarray(np.array(state, \"uint8\").reshape(self.size, self.size, 3))\n",
    "\n",
    "    def _receive(self):\n",
    "        # Kudos to Jan for the socket.MSG_WAITALL fix!\n",
    "        data   = self.client.recv(2 + 3 * self.size ** 2, socket.MSG_WAITALL)\n",
    "        end    = data[0]\n",
    "        reward = data[1]\n",
    "        state  = [data[i] for i in range(2, len(data))]\n",
    "\n",
    "        return end, reward, state\n",
    "\n",
    "    def _send(self, action, command):\n",
    "        self.client.send(bytes([action, command]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LrQ9Wpkctbz",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# These are the default environment arguments. They must be the same as the values that are set in the environment GUI.\n",
    "ip         = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to\n",
    "port       = 13000       # Port number that the TCP/IP interface listens to\n",
    "size       = 50         # Please check the Updates section above for more details\n",
    "timescale  = 10           # Please check the Updates section above for more details\n",
    "\n",
    "agent = Neurosmash_Agent() # This is an example agent.\n",
    "                           # It has a step function, which gets reward/state as arguments and returns an action.\n",
    "                           # Right now, it always outputs a random action (3) regardless of reward/state.\n",
    "                           # The real agent should output one of the following three actions:\n",
    "                           # none (0), left (1) and right (2)\n",
    "\n",
    "environment = Neurosmash_Environment(ip=ip, port=port, size=size, timescale=timescale) # This is the main environment.\n",
    "                                       # It has a reset function, which is used to reset the environment before episodes.\n",
    "                                       # It also has a step function, which is used to which steps one time point\n",
    "                                       # It gets an action (as defined above) as input and outputs the following:\n",
    "                                       # end (true if the episode has ended, false otherwise)\n",
    "                                       # reward (10 if won, 0 otherwise)\n",
    "                                       # state (flattened size x size x 3 vector of pixel values)\n",
    "                                       # The state can be converted into an image as follows:\n",
    "                                       # image = np.array(state, \"uint8\").reshape(size, size, 3)\n",
    "                                       # You can also use to Neurosmash.Environment.state2image(state) function which returns\n",
    "                                       # the state as a PIL image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# end, reward, state = environment.reset()\n",
    "\n",
    "# screen = environment.state2image(state)\n",
    "# plt.imshow(screen)\n",
    "# screen_shape = np.shape(screen)\n",
    "# nr_actions = 3\n",
    "\n",
    "# dqn = DQN(screen_shape[0], screen_shape[1], nr_actions)\n",
    "\n",
    "# #print(np.shape(environment.state2image(state)))\n",
    "\n",
    "# # Transpose to pytorch order of dimensions (cwh)\n",
    "# screen = np.transpose(screen, axes=(2,0,1))\n",
    "# # Translate to rgb float values\n",
    "# screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "# # Convert to tensor\n",
    "# screen = torch.from_numpy(screen)\n",
    "# # Add batch dimension, yeet to device\n",
    "# screen = screen.unsqueeze(0).to(device)\n",
    "\n",
    "# result = dqn(screen)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Initialize environment, gather initial state\n",
    "end, reward, state = environment.reset()\n",
    "\n",
    "screen = environment.state2image(state)\n",
    "plt.imshow(screen)\n",
    "screen_shape = np.shape(screen)\n",
    "n_actions = 3\n",
    "\n",
    "#TODO: make a direct state2input function\n",
    "def image2input(screen):\n",
    "    \"\"\" Converts an image gained from the environment (through environment.state2image) to ready-to-use input\"\"\"\n",
    "    # Transpose to pytorch order of dimensions (cwh)\n",
    "    screen = np.transpose(screen, axes=(2,0,1))\n",
    "    # Translate to rgb float values\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # Convert to tensor\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Add batch dimension, yeet to device\n",
    "    screen = screen.unsqueeze(0).to(device)\n",
    "    return screen\n",
    "    \n",
    "\n",
    "# Initialize DQN network\n",
    "policy_net = DQN(screen_shape[0], screen_shape[1], n_actions).to(device)\n",
    "target_net = DQN(screen_shape[0], screen_shape[1], n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 completed in 1.3986947536468506\n",
      "episode 1 completed in 1.6108155250549316\n",
      "episode 2 completed in 2.8835437297821045\n",
      "episode 3 completed in 1.9996163845062256\n",
      "episode 4 completed in 3.0704574584960938\n",
      "episode 5 completed in 6.948839902877808\n",
      "episode 6 completed in 5.101681470870972\n",
      "Victory!!!!\n",
      "episode 7 completed in 8.944677591323853\n",
      "episode 8 completed in 18.538522243499756\n",
      "Victory!!!!\n",
      "episode 9 completed in 23.470104455947876\n",
      "Victory!!!!\n",
      "episode 10 completed in 16.842403650283813\n",
      "episode 11 completed in 25.122615814208984\n",
      "Victory!!!!\n",
      "episode 12 completed in 26.74729609489441\n",
      "episode 13 completed in 7.003573179244995\n",
      "Victory!!!!\n",
      "episode 14 completed in 11.746132135391235\n",
      "episode 15 completed in 15.368261337280273\n",
      "episode 16 completed in 6.802236795425415\n",
      "episode 17 completed in 14.952241659164429\n",
      "episode 18 completed in 28.332226991653442\n",
      "episode 19 completed in 6.906880140304565\n",
      "episode 20 completed in 8.089528560638428\n",
      "episode 21 completed in 20.5333833694458\n",
      "episode 22 completed in 9.40680718421936\n",
      "episode 23 completed in 6.9418251514434814\n",
      "episode 24 completed in 25.663665771484375\n",
      "episode 25 completed in 11.508118391036987\n",
      "episode 26 completed in 5.478272914886475\n",
      "episode 27 completed in 6.973362684249878\n",
      "episode 28 completed in 9.107127666473389\n",
      "Victory!!!!\n",
      "episode 29 completed in 12.106446027755737\n",
      "episode 30 completed in 11.831252098083496\n",
      "episode 31 completed in 5.8808677196502686\n",
      "episode 32 completed in 55.994165897369385\n",
      "episode 33 completed in 7.815985441207886\n",
      "episode 34 completed in 7.247260570526123\n",
      "episode 35 completed in 9.467491149902344\n",
      "episode 36 completed in 11.577922582626343\n",
      "Victory!!!!\n",
      "episode 37 completed in 10.496918439865112\n",
      "episode 38 completed in 9.663604974746704\n",
      "Victory!!!!\n",
      "episode 39 completed in 17.19891381263733\n",
      "Victory!!!!\n",
      "episode 40 completed in 112.10190200805664\n",
      "episode 41 completed in 28.864912033081055\n",
      "episode 42 completed in 73.34522080421448\n",
      "Victory!!!!\n",
      "episode 43 completed in 36.76581597328186\n",
      "episode 44 completed in 10.646513938903809\n",
      "Victory!!!!\n",
      "episode 45 completed in 7.1245129108428955\n",
      "Victory!!!!\n",
      "episode 46 completed in 83.96024203300476\n",
      "Victory!!!!\n",
      "episode 47 completed in 32.23220133781433\n",
      "Victory!!!!\n",
      "episode 48 completed in 8.070236921310425\n",
      "Victory!!!!\n",
      "episode 49 completed in 6.763995885848999\n",
      "Victory!!!!\n",
      "episode 50 completed in 52.501075983047485\n",
      "Victory!!!!\n",
      "episode 51 completed in 66.75206685066223\n",
      "Victory!!!!\n",
      "episode 52 completed in 8.042969942092896\n",
      "Victory!!!!\n",
      "episode 53 completed in 8.980771541595459\n",
      "Victory!!!!\n",
      "episode 54 completed in 16.193313360214233\n",
      "Victory!!!!\n",
      "episode 55 completed in 36.011908769607544\n",
      "Victory!!!!\n",
      "episode 56 completed in 9.15549635887146\n",
      "Victory!!!!\n",
      "episode 57 completed in 7.896751165390015\n",
      "episode 58 completed in 12.963333129882812\n",
      "Victory!!!!\n",
      "episode 59 completed in 22.422025203704834\n",
      "Victory!!!!\n",
      "episode 60 completed in 8.504895210266113\n",
      "Victory!!!!\n",
      "episode 61 completed in 6.324004888534546\n",
      "episode 62 completed in 64.00713562965393\n",
      "Victory!!!!\n",
      "episode 63 completed in 6.472954273223877\n",
      "episode 64 completed in 10.331726551055908\n",
      "episode 65 completed in 20.534858226776123\n",
      "Victory!!!!\n",
      "episode 66 completed in 9.231975793838501\n",
      "episode 67 completed in 30.59001851081848\n",
      "Victory!!!!\n",
      "episode 68 completed in 83.39479947090149\n",
      "episode 69 completed in 46.13535022735596\n",
      "Victory!!!!\n",
      "episode 70 completed in 24.165834665298462\n",
      "Victory!!!!\n",
      "episode 71 completed in 114.88823747634888\n",
      "episode 72 completed in 139.0863480567932\n",
      "Victory!!!!\n",
      "episode 73 completed in 34.798449754714966\n",
      "Victory!!!!\n",
      "episode 74 completed in 11.96923017501831\n",
      "Victory!!!!\n",
      "episode 75 completed in 33.42569041252136\n",
      "Victory!!!!\n",
      "episode 76 completed in 8.537658929824829\n",
      "Victory!!!!\n",
      "episode 77 completed in 18.108309984207153\n",
      "Victory!!!!\n",
      "episode 78 completed in 18.628466606140137\n",
      "episode 79 completed in 22.762107849121094\n",
      "episode 80 completed in 15.375040769577026\n",
      "Victory!!!!\n",
      "episode 81 completed in 17.891677856445312\n",
      "Victory!!!!\n",
      "episode 82 completed in 82.21911287307739\n",
      "Victory!!!!\n",
      "episode 83 completed in 40.06622076034546\n",
      "episode 84 completed in 93.66398644447327\n",
      "Victory!!!!\n",
      "episode 85 completed in 8.12680196762085\n",
      "Victory!!!!\n",
      "episode 86 completed in 12.790639162063599\n",
      "Victory!!!!\n",
      "episode 87 completed in 12.765040636062622\n",
      "Victory!!!!\n",
      "episode 88 completed in 111.80304956436157\n",
      "Victory!!!!\n",
      "episode 89 completed in 19.789104223251343\n",
      "Victory!!!!\n",
      "episode 90 completed in 7.757657051086426\n",
      "Victory!!!!\n",
      "episode 91 completed in 50.01905417442322\n",
      "Victory!!!!\n",
      "episode 92 completed in 24.739665985107422\n",
      "episode 93 completed in 14.61156439781189\n",
      "Victory!!!!\n",
      "episode 94 completed in 8.460562467575073\n",
      "Victory!!!!\n",
      "episode 95 completed in 66.59526824951172\n",
      "episode 96 completed in 43.37091946601868\n",
      "Victory!!!!\n",
      "episode 97 completed in 105.06390190124512\n",
      "Victory!!!!\n",
      "episode 98 completed in 42.21473550796509\n",
      "Victory!!!!\n",
      "episode 99 completed in 74.99021005630493\n",
      "Victory!!!!\n",
      "episode 100 completed in 13.19493293762207\n",
      "episode 101 completed in 21.23884630203247\n",
      "Victory!!!!\n",
      "episode 102 completed in 21.539202213287354\n",
      "Victory!!!!\n",
      "episode 103 completed in 46.98912453651428\n",
      "Victory!!!!\n",
      "episode 104 completed in 12.462457418441772\n",
      "episode 105 completed in 11.501797437667847\n",
      "Victory!!!!\n",
      "episode 106 completed in 9.051199913024902\n",
      "Victory!!!!\n",
      "episode 107 completed in 20.801973342895508\n",
      "Victory!!!!\n",
      "episode 108 completed in 10.30038070678711\n",
      "episode 109 completed in 124.31580138206482\n",
      "Victory!!!!\n",
      "episode 110 completed in 11.529955387115479\n",
      "Victory!!!!\n",
      "episode 111 completed in 27.733888626098633\n",
      "episode 112 completed in 41.29962944984436\n",
      "Victory!!!!\n",
      "episode 113 completed in 25.336575746536255\n",
      "Victory!!!!\n",
      "episode 114 completed in 11.373965978622437\n",
      "Victory!!!!\n",
      "episode 115 completed in 7.247701406478882\n",
      "episode 116 completed in 26.60872983932495\n",
      "Victory!!!!\n",
      "episode 117 completed in 106.42241048812866\n",
      "Victory!!!!\n",
      "episode 118 completed in 8.30827283859253\n",
      "Victory!!!!\n",
      "episode 119 completed in 13.896944284439087\n",
      "Victory!!!!\n",
      "episode 120 completed in 106.21987247467041\n",
      "Victory!!!!\n",
      "episode 121 completed in 11.475763320922852\n",
      "episode 122 completed in 30.379684448242188\n",
      "Victory!!!!\n",
      "episode 123 completed in 55.26031279563904\n",
      "Victory!!!!\n",
      "episode 124 completed in 57.37571740150452\n",
      "episode 125 completed in 107.1917040348053\n",
      "Victory!!!!\n",
      "episode 126 completed in 16.98022437095642\n",
      "episode 127 completed in 35.813685178756714\n",
      "episode 128 completed in 81.26321935653687\n",
      "Victory!!!!\n",
      "episode 129 completed in 45.389920234680176\n",
      "Victory!!!!\n",
      "episode 130 completed in 31.901079893112183\n",
      "Victory!!!!\n",
      "episode 131 completed in 12.672031164169312\n",
      "Victory!!!!\n",
      "episode 132 completed in 88.81696343421936\n",
      "episode 133 completed in 24.012778759002686\n",
      "episode 134 completed in 18.543154001235962\n",
      "Victory!!!!\n",
      "episode 135 completed in 14.675806522369385\n",
      "Victory!!!!\n",
      "episode 136 completed in 13.631075382232666\n",
      "episode 137 completed in 34.3339364528656\n",
      "episode 138 completed in 11.700393438339233\n",
      "episode 139 completed in 61.24918031692505\n",
      "Victory!!!!\n",
      "episode 140 completed in 62.33710765838623\n",
      "episode 141 completed in 18.867533206939697\n",
      "Victory!!!!\n",
      "episode 142 completed in 49.59489941596985\n",
      "Victory!!!!\n",
      "episode 143 completed in 11.897923231124878\n",
      "Victory!!!!\n",
      "episode 144 completed in 8.45619797706604\n",
      "Victory!!!!\n",
      "episode 145 completed in 37.53800916671753\n",
      "Victory!!!!\n",
      "episode 146 completed in 18.20629596710205\n",
      "Victory!!!!\n",
      "episode 147 completed in 16.448779344558716\n",
      "Victory!!!!\n",
      "episode 148 completed in 7.94583797454834\n",
      "episode 149 completed in 16.583780765533447\n",
      "Victory!!!!\n",
      "episode 150 completed in 37.55949640274048\n",
      "episode 151 completed in 31.37096381187439\n",
      "episode 152 completed in 16.223886251449585\n",
      "episode 153 completed in 21.67763042449951\n",
      "Victory!!!!\n",
      "episode 154 completed in 22.991501808166504\n",
      "Victory!!!!\n",
      "episode 155 completed in 5.267959117889404\n",
      "episode 156 completed in 39.32831120491028\n",
      "Victory!!!!\n",
      "episode 157 completed in 8.63611102104187\n",
      "episode 158 completed in 28.60026741027832\n",
      "Victory!!!!\n",
      "episode 159 completed in 135.34060955047607\n",
      "episode 160 completed in 10.834550380706787\n",
      "episode 161 completed in 8.45367693901062\n",
      "Victory!!!!\n",
      "episode 162 completed in 5.904745817184448\n",
      "episode 163 completed in 15.846351385116577\n",
      "Victory!!!!\n",
      "episode 164 completed in 18.241960763931274\n",
      "episode 165 completed in 12.456071376800537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victory!!!!\n",
      "episode 166 completed in 32.130215644836426\n",
      "episode 167 completed in 38.043896198272705\n",
      "Victory!!!!\n",
      "episode 168 completed in 39.42726540565491\n",
      "episode 169 completed in 15.448798894882202\n",
      "Victory!!!!\n",
      "episode 170 completed in 26.966196537017822\n",
      "episode 171 completed in 14.60601544380188\n",
      "episode 172 completed in 13.2947256565094\n",
      "Victory!!!!\n",
      "episode 173 completed in 19.58152413368225\n",
      "episode 174 completed in 24.299707174301147\n",
      "Victory!!!!\n",
      "episode 175 completed in 23.379218578338623\n",
      "Victory!!!!\n",
      "episode 176 completed in 35.69447183609009\n",
      "Victory!!!!\n",
      "episode 177 completed in 5.617310523986816\n",
      "episode 178 completed in 58.41689968109131\n",
      "Victory!!!!\n",
      "episode 179 completed in 7.598801851272583\n",
      "episode 180 completed in 59.302894830703735\n",
      "Victory!!!!\n",
      "episode 181 completed in 13.820373058319092\n",
      "Victory!!!!\n",
      "episode 182 completed in 11.207303762435913\n",
      "episode 183 completed in 6.763291597366333\n",
      "episode 184 completed in 3.0862951278686523\n",
      "Victory!!!!\n",
      "episode 185 completed in 63.34251666069031\n",
      "Victory!!!!\n",
      "episode 186 completed in 7.083022832870483\n",
      "episode 187 completed in 8.090953588485718\n",
      "Victory!!!!\n",
      "episode 188 completed in 6.815543174743652\n",
      "Victory!!!!\n",
      "episode 189 completed in 5.710968971252441\n",
      "episode 190 completed in 18.193150281906128\n",
      "Victory!!!!\n",
      "episode 191 completed in 9.448123931884766\n",
      "Victory!!!!\n",
      "episode 192 completed in 48.70243430137634\n",
      "episode 193 completed in 12.531998872756958\n",
      "episode 194 completed in 25.54103183746338\n",
      "episode 195 completed in 26.092414617538452\n",
      "episode 196 completed in 9.353843688964844\n",
      "Victory!!!!\n",
      "episode 197 completed in 6.740450620651245\n",
      "episode 198 completed in 11.94634747505188\n",
      "Victory!!!!\n",
      "episode 199 completed in 25.426696062088013\n",
      "episode 200 completed in 4.745575189590454\n",
      "episode 201 completed in 21.641961812973022\n",
      "episode 202 completed in 3.9350106716156006\n",
      "episode 203 completed in 7.390709400177002\n",
      "Victory!!!!\n",
      "episode 204 completed in 26.976176977157593\n",
      "episode 205 completed in 14.700471639633179\n",
      "Victory!!!!\n",
      "episode 206 completed in 6.07463526725769\n",
      "episode 207 completed in 48.62354779243469\n",
      "episode 208 completed in 5.3526740074157715\n",
      "episode 209 completed in 10.072921752929688\n",
      "episode 210 completed in 3.7520596981048584\n",
      "Victory!!!!\n",
      "episode 211 completed in 35.807700395584106\n",
      "Victory!!!!\n",
      "episode 212 completed in 5.807518720626831\n",
      "Victory!!!!\n",
      "episode 213 completed in 28.36371612548828\n",
      "episode 214 completed in 6.450032711029053\n",
      "episode 215 completed in 13.292568683624268\n",
      "episode 216 completed in 5.217848062515259\n",
      "episode 217 completed in 10.846383094787598\n",
      "Victory!!!!\n",
      "episode 218 completed in 9.096710681915283\n",
      "episode 219 completed in 8.344894170761108\n",
      "episode 220 completed in 6.952069997787476\n",
      "episode 221 completed in 4.351969003677368\n",
      "episode 222 completed in 3.298349142074585\n",
      "Victory!!!!\n",
      "episode 223 completed in 21.314629077911377\n",
      "episode 224 completed in 27.086413860321045\n",
      "episode 225 completed in 14.51263427734375\n",
      "Victory!!!!\n",
      "episode 226 completed in 8.914788007736206\n",
      "Victory!!!!\n",
      "episode 227 completed in 7.290722846984863\n",
      "episode 228 completed in 12.649252891540527\n",
      "Victory!!!!\n",
      "episode 229 completed in 21.162006616592407\n",
      "episode 230 completed in 8.862728595733643\n",
      "episode 231 completed in 4.958661794662476\n",
      "Victory!!!!\n",
      "episode 232 completed in 7.296036243438721\n",
      "episode 233 completed in 4.049252986907959\n",
      "Victory!!!!\n",
      "episode 234 completed in 8.261773347854614\n",
      "episode 235 completed in 3.8642067909240723\n",
      "Victory!!!!\n",
      "episode 236 completed in 11.84404706954956\n",
      "episode 237 completed in 3.7738845348358154\n",
      "Victory!!!!\n",
      "episode 238 completed in 11.982298612594604\n",
      "episode 239 completed in 18.950617790222168\n",
      "episode 240 completed in 29.027838230133057\n",
      "episode 241 completed in 4.107810020446777\n",
      "episode 242 completed in 3.850881576538086\n",
      "episode 243 completed in 27.98035478591919\n",
      "episode 244 completed in 20.40414595603943\n",
      "Victory!!!!\n",
      "episode 245 completed in 9.233107328414917\n",
      "episode 246 completed in 27.519524812698364\n",
      "episode 247 completed in 14.851304292678833\n",
      "episode 248 completed in 4.335330009460449\n",
      "episode 249 completed in 18.78294587135315\n",
      "episode 250 completed in 44.54011654853821\n",
      "Victory!!!!\n",
      "episode 251 completed in 11.958491325378418\n",
      "episode 252 completed in 5.067298412322998\n",
      "Victory!!!!\n",
      "episode 253 completed in 20.82818078994751\n",
      "episode 254 completed in 3.609118938446045\n",
      "episode 255 completed in 6.04860258102417\n",
      "episode 256 completed in 4.608671188354492\n",
      "episode 257 completed in 4.6515185832977295\n",
      "episode 258 completed in 17.902686834335327\n",
      "episode 259 completed in 3.853240489959717\n",
      "episode 260 completed in 7.0714404582977295\n",
      "episode 261 completed in 3.952423095703125\n",
      "episode 262 completed in 7.067466974258423\n",
      "episode 263 completed in 4.4282450675964355\n",
      "episode 264 completed in 3.8846802711486816\n",
      "Victory!!!!\n",
      "episode 265 completed in 20.372164249420166\n",
      "episode 266 completed in 7.808285713195801\n",
      "episode 267 completed in 16.98802900314331\n",
      "episode 268 completed in 44.70346665382385\n",
      "episode 269 completed in 33.29029393196106\n",
      "episode 270 completed in 32.09607219696045\n",
      "episode 271 completed in 5.634087562561035\n",
      "episode 272 completed in 9.890922784805298\n",
      "Victory!!!!\n",
      "episode 273 completed in 11.994163990020752\n",
      "Victory!!!!\n",
      "episode 274 completed in 18.49455952644348\n",
      "episode 275 completed in 7.0127105712890625\n",
      "episode 276 completed in 3.3351471424102783\n",
      "Victory!!!!\n",
      "episode 277 completed in 31.86173725128174\n",
      "episode 278 completed in 3.5420358180999756\n",
      "episode 279 completed in 16.472450733184814\n",
      "episode 280 completed in 3.765180826187134\n",
      "episode 281 completed in 3.520799398422241\n",
      "Victory!!!!\n",
      "episode 282 completed in 16.739611625671387\n",
      "Victory!!!!\n",
      "episode 283 completed in 7.7854344844818115\n",
      "Victory!!!!\n",
      "episode 284 completed in 33.58664131164551\n",
      "episode 285 completed in 39.19884181022644\n",
      "episode 286 completed in 7.504446268081665\n",
      "episode 287 completed in 3.701004981994629\n",
      "episode 288 completed in 18.227598428726196\n",
      "episode 289 completed in 4.0962793827056885\n",
      "episode 290 completed in 9.754438877105713\n",
      "episode 291 completed in 5.324195623397827\n",
      "episode 292 completed in 11.86909532546997\n",
      "episode 293 completed in 3.785346031188965\n",
      "Victory!!!!\n",
      "episode 294 completed in 14.744885206222534\n",
      "episode 295 completed in 5.183844566345215\n",
      "Victory!!!!\n",
      "episode 296 completed in 11.070826053619385\n",
      "episode 297 completed in 3.4435410499572754\n",
      "Victory!!!!\n",
      "episode 298 completed in 19.305940866470337\n",
      "episode 299 completed in 7.480669260025024\n",
      "episode 300 completed in 32.1258819103241\n",
      "Victory!!!!\n",
      "episode 301 completed in 11.140892505645752\n",
      "Victory!!!!\n",
      "episode 302 completed in 8.41275930404663\n",
      "Victory!!!!\n",
      "episode 303 completed in 13.536609649658203\n",
      "episode 304 completed in 28.213404417037964\n",
      "episode 305 completed in 7.562862157821655\n",
      "episode 306 completed in 3.2485883235931396\n",
      "episode 307 completed in 10.103863954544067\n",
      "episode 308 completed in 8.805849075317383\n",
      "Victory!!!!\n",
      "episode 309 completed in 35.21428632736206\n",
      "episode 310 completed in 17.857616901397705\n",
      "episode 311 completed in 6.965672492980957\n",
      "episode 312 completed in 3.4171223640441895\n",
      "Victory!!!!\n",
      "episode 313 completed in 57.331977128982544\n",
      "Victory!!!!\n",
      "episode 314 completed in 17.594279527664185\n",
      "episode 315 completed in 3.088083505630493\n",
      "episode 316 completed in 3.373380661010742\n",
      "Victory!!!!\n",
      "episode 317 completed in 17.950963497161865\n",
      "episode 318 completed in 3.7263312339782715\n",
      "episode 319 completed in 6.9977710247039795\n",
      "Victory!!!!\n",
      "episode 320 completed in 58.14658999443054\n",
      "episode 321 completed in 23.72219491004944\n",
      "Victory!!!!\n",
      "episode 322 completed in 6.310643196105957\n",
      "episode 323 completed in 13.435394287109375\n",
      "episode 324 completed in 3.132502555847168\n",
      "episode 325 completed in 10.553443908691406\n",
      "episode 326 completed in 7.097808122634888\n",
      "episode 327 completed in 15.128873348236084\n",
      "episode 328 completed in 10.921348810195923\n",
      "Victory!!!!\n",
      "episode 329 completed in 8.680352449417114\n",
      "episode 330 completed in 3.1239352226257324\n",
      "episode 331 completed in 21.52449941635132\n",
      "episode 332 completed in 33.857043504714966\n",
      "episode 333 completed in 6.476515054702759\n",
      "episode 334 completed in 10.391284704208374\n",
      "episode 335 completed in 11.77174687385559\n",
      "episode 336 completed in 3.1294798851013184\n",
      "Victory!!!!\n",
      "episode 337 completed in 5.5727105140686035\n",
      "Victory!!!!\n",
      "episode 338 completed in 6.586306810379028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 339 completed in 3.7734622955322266\n",
      "episode 340 completed in 3.500509262084961\n",
      "episode 341 completed in 3.394674062728882\n",
      "episode 342 completed in 3.6516308784484863\n",
      "episode 343 completed in 4.180718183517456\n",
      "episode 344 completed in 21.920175790786743\n",
      "episode 345 completed in 12.825370788574219\n",
      "episode 346 completed in 3.7249131202697754\n",
      "episode 347 completed in 4.7747485637664795\n",
      "episode 348 completed in 3.474031686782837\n",
      "episode 349 completed in 3.7831923961639404\n",
      "Victory!!!!\n",
      "episode 350 completed in 11.626120328903198\n",
      "episode 351 completed in 9.31025242805481\n",
      "Victory!!!!\n",
      "episode 352 completed in 7.004469633102417\n",
      "Victory!!!!\n",
      "episode 353 completed in 17.5220685005188\n",
      "episode 354 completed in 12.007473707199097\n",
      "episode 355 completed in 3.7705447673797607\n",
      "Victory!!!!\n",
      "episode 356 completed in 21.140581130981445\n",
      "Victory!!!!\n",
      "episode 357 completed in 19.46133589744568\n",
      "episode 358 completed in 3.794543504714966\n",
      "episode 359 completed in 3.5961389541625977\n",
      "episode 360 completed in 3.7627663612365723\n",
      "episode 361 completed in 64.03223824501038\n",
      "episode 362 completed in 11.153122186660767\n",
      "episode 363 completed in 12.00482726097107\n",
      "episode 364 completed in 6.723532676696777\n",
      "episode 365 completed in 26.255427360534668\n",
      "episode 366 completed in 5.058578252792358\n",
      "episode 367 completed in 3.4871222972869873\n",
      "episode 368 completed in 9.832932472229004\n",
      "episode 369 completed in 18.39319086074829\n",
      "episode 370 completed in 3.723252058029175\n",
      "episode 371 completed in 3.487031936645508\n",
      "episode 372 completed in 15.93173360824585\n",
      "episode 373 completed in 4.076778888702393\n",
      "episode 374 completed in 6.606377601623535\n",
      "episode 375 completed in 11.729534149169922\n",
      "episode 376 completed in 3.476198196411133\n",
      "Victory!!!!\n",
      "episode 377 completed in 6.296645879745483\n",
      "episode 378 completed in 5.54478120803833\n",
      "episode 379 completed in 6.015745639801025\n",
      "episode 380 completed in 12.201286792755127\n",
      "episode 381 completed in 9.181684494018555\n",
      "episode 382 completed in 3.347766160964966\n",
      "episode 383 completed in 3.405731678009033\n",
      "episode 384 completed in 7.69983696937561\n",
      "episode 385 completed in 4.091737270355225\n",
      "episode 386 completed in 11.312148332595825\n",
      "episode 387 completed in 9.822469472885132\n",
      "episode 388 completed in 7.365234136581421\n",
      "episode 389 completed in 6.723814487457275\n",
      "Victory!!!!\n",
      "episode 390 completed in 6.456820487976074\n",
      "episode 391 completed in 8.612840175628662\n",
      "episode 392 completed in 3.6942975521087646\n",
      "episode 393 completed in 4.428912162780762\n",
      "episode 394 completed in 3.936002731323242\n",
      "episode 395 completed in 8.030794143676758\n",
      "Victory!!!!\n",
      "episode 396 completed in 14.461799621582031\n",
      "episode 397 completed in 15.731653451919556\n",
      "episode 398 completed in 28.87431311607361\n",
      "Victory!!!!\n",
      "episode 399 completed in 10.122279405593872\n",
      "episode 400 completed in 3.571469783782959\n",
      "episode 401 completed in 7.406072616577148\n",
      "Victory!!!!\n",
      "episode 402 completed in 7.627683639526367\n",
      "episode 403 completed in 17.638005256652832\n",
      "episode 404 completed in 3.5949909687042236\n",
      "episode 405 completed in 8.845454454421997\n",
      "Victory!!!!\n",
      "episode 406 completed in 17.429253816604614\n",
      "episode 407 completed in 6.069279432296753\n",
      "episode 408 completed in 5.782566547393799\n",
      "episode 409 completed in 6.381716728210449\n",
      "episode 410 completed in 7.378870010375977\n",
      "episode 411 completed in 13.326112270355225\n",
      "Victory!!!!\n",
      "episode 412 completed in 15.206496000289917\n",
      "episode 413 completed in 4.181900262832642\n",
      "Victory!!!!\n",
      "episode 414 completed in 18.378965854644775\n",
      "episode 415 completed in 9.689180612564087\n",
      "episode 416 completed in 3.447819709777832\n",
      "episode 417 completed in 4.407075643539429\n",
      "episode 418 completed in 7.494099855422974\n",
      "episode 419 completed in 3.156844139099121\n",
      "episode 420 completed in 4.700265884399414\n",
      "episode 421 completed in 3.6367757320404053\n",
      "episode 422 completed in 3.6160664558410645\n",
      "episode 423 completed in 4.251232385635376\n",
      "episode 424 completed in 16.116249561309814\n",
      "episode 425 completed in 4.134907960891724\n",
      "episode 426 completed in 6.760093688964844\n",
      "episode 427 completed in 5.964666366577148\n",
      "episode 428 completed in 3.9117398262023926\n",
      "episode 429 completed in 4.260003566741943\n",
      "episode 430 completed in 5.0530195236206055\n",
      "episode 431 completed in 4.727992296218872\n",
      "episode 432 completed in 25.235188961029053\n",
      "episode 433 completed in 3.6063477993011475\n",
      "episode 434 completed in 7.8839592933654785\n",
      "Victory!!!!\n",
      "episode 435 completed in 19.17990493774414\n",
      "Victory!!!!\n",
      "episode 436 completed in 19.053458213806152\n",
      "Victory!!!!\n",
      "episode 437 completed in 14.303685188293457\n",
      "episode 438 completed in 10.855889558792114\n",
      "episode 439 completed in 6.819783687591553\n",
      "episode 440 completed in 3.5983831882476807\n",
      "episode 441 completed in 3.3401899337768555\n",
      "episode 442 completed in 26.22633647918701\n",
      "Victory!!!!\n",
      "episode 443 completed in 32.01058101654053\n",
      "episode 444 completed in 3.6217713356018066\n",
      "episode 445 completed in 5.755850076675415\n",
      "episode 446 completed in 8.918222427368164\n",
      "episode 447 completed in 23.320581436157227\n",
      "episode 448 completed in 16.865258932113647\n",
      "episode 449 completed in 30.554403066635132\n",
      "episode 450 completed in 4.5834877490997314\n",
      "episode 451 completed in 5.939720153808594\n",
      "Victory!!!!\n",
      "episode 452 completed in 24.86675214767456\n",
      "episode 453 completed in 17.89590620994568\n",
      "episode 454 completed in 20.06228256225586\n",
      "episode 455 completed in 28.2562198638916\n",
      "episode 456 completed in 22.735447645187378\n",
      "episode 457 completed in 4.252386569976807\n",
      "episode 458 completed in 6.526010990142822\n",
      "episode 459 completed in 29.209548234939575\n",
      "Victory!!!!\n",
      "episode 460 completed in 24.86657476425171\n",
      "Victory!!!!\n",
      "episode 461 completed in 9.851065874099731\n",
      "episode 462 completed in 25.26682949066162\n",
      "episode 463 completed in 3.7821969985961914\n",
      "episode 464 completed in 7.783844470977783\n",
      "episode 465 completed in 4.114033937454224\n",
      "episode 466 completed in 4.850834369659424\n",
      "Victory!!!!\n",
      "episode 467 completed in 40.77284336090088\n",
      "episode 468 completed in 3.6685078144073486\n",
      "episode 469 completed in 10.988287448883057\n",
      "episode 470 completed in 76.87278723716736\n",
      "episode 471 completed in 28.69457745552063\n",
      "episode 472 completed in 3.916398763656616\n",
      "episode 473 completed in 3.9240987300872803\n",
      "episode 474 completed in 7.8199121952056885\n",
      "episode 475 completed in 3.303201198577881\n",
      "episode 476 completed in 50.37624645233154\n",
      "episode 477 completed in 3.8480751514434814\n",
      "episode 478 completed in 22.85450553894043\n",
      "episode 479 completed in 24.242510557174683\n",
      "episode 480 completed in 23.284259796142578\n",
      "episode 481 completed in 10.813417196273804\n",
      "episode 482 completed in 8.877092599868774\n",
      "episode 483 completed in 25.982563495635986\n",
      "episode 484 completed in 12.145745277404785\n",
      "episode 485 completed in 51.29600262641907\n",
      "episode 486 completed in 8.771017789840698\n",
      "episode 487 completed in 4.244096755981445\n",
      "episode 488 completed in 7.145635366439819\n",
      "episode 489 completed in 4.764966011047363\n",
      "episode 490 completed in 22.045897006988525\n",
      "Victory!!!!\n",
      "episode 491 completed in 7.624379873275757\n",
      "episode 492 completed in 9.118242979049683\n",
      "episode 493 completed in 15.645260095596313\n",
      "episode 494 completed in 4.46706748008728\n",
      "episode 495 completed in 7.115644216537476\n",
      "Victory!!!!\n",
      "episode 496 completed in 9.970656633377075\n",
      "episode 497 completed in 30.781947374343872\n",
      "episode 498 completed in 10.743752002716064\n",
      "episode 499 completed in 10.286969423294067\n",
      "episode 500 completed in 6.656467914581299\n",
      "episode 501 completed in 5.021461248397827\n",
      "episode 502 completed in 5.472048044204712\n",
      "Victory!!!!\n",
      "episode 503 completed in 17.917754411697388\n",
      "episode 504 completed in 4.520464181900024\n",
      "episode 505 completed in 10.837883949279785\n",
      "episode 506 completed in 3.809216260910034\n",
      "episode 507 completed in 11.716652631759644\n",
      "episode 508 completed in 3.5666418075561523\n",
      "Victory!!!!\n",
      "episode 509 completed in 32.970234394073486\n",
      "episode 510 completed in 3.457709312438965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d67702b60ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Select and perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Victory!!!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-98da801ad4a1>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-98da801ad4a1>\u001b[0m in \u001b[0;36m_receive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Kudos to Jan for the socket.MSG_WAITALL fix!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdata\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSG_WAITALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mend\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 10_000\n",
    "steps_done = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    timer = time.time()\n",
    "    \n",
    "    # Initialize the environment and state\n",
    "    end, reward, state = environment.reset()\n",
    "    state = image2input(environment.state2image(state))\n",
    "    last_state = state\n",
    "    current_state = state\n",
    "    state = current_state - last_state\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        end, reward, state = environment.step(action.item())\n",
    "        if reward == 10:\n",
    "            print(\"Victory!!!!\")\n",
    "        #reward += 20\n",
    "        state = image2input(environment.state2image(state))\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_state = current_state\n",
    "        current_state = state\n",
    "        if not end:\n",
    "            next_state = current_state - last_state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if end:\n",
    "            episode_durations.append(t + 1)\n",
    "            #plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    print(\"episode\", i_episode, \"completed in\", time.time() - timer)\n",
    "    \n",
    "    if (i_episode+1)%500 == 0:\n",
    "        torch.save(policy_net.state_dict(), \"dict_e\" + str(i_episode+1) +\".kek\")\n",
    "        \n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "# env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Together with next cell, test function to compute difference between last and current screen\n",
    "\n",
    "end, reward, state = environment.reset()\n",
    "\n",
    "last_screen = environment.state2image(state)\n",
    "curr_screen = environment.state2image(environment.step(0)[2])\n",
    "diff = ImageChops.difference(curr_screen, last_screen)\n",
    "plt.imshow(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Together with previous cell, test function to compute difference between last and current screen\n",
    "last_screen = curr_screen\n",
    "curr_screen = environment.state2image(environment.step(0)[2])\n",
    "diff = ImageChops.difference(curr_screen, last_screen)\n",
    "plt.imshow(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rc0CixCkctc2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "README.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
